{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOj2oS2WoE1xhfp1YmmpMIz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarceloNevesDS/MarceloNevesDS/blob/main/Desafio_Final_Bootcamp_EDC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Instalação: JAVA, HADOOP e SPARK**"
      ],
      "metadata": {
        "id": "CTABAs4Coph3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalando o Java\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "metadata": {
        "id": "Gk2Jtu2foya0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fazendo download Hadoop e Spark\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop2.7.tgz\n",
        "\n",
        "# Descompactando os arquivos\n",
        "!tar xf spark-3.1.2-bin-hadoop2.7.tgz"
      ],
      "metadata": {
        "id": "FbOwDmljo2Ac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importando a biblioteca os\n",
        "import os\n",
        "\n",
        "# Definindo a variável de ambiente para o Java\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "\n",
        "# Definindo a variável de ambiente para o Spark\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop2.7\""
      ],
      "metadata": {
        "id": "slXk7S8ho2C_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# instalando findspark\n",
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "QjevjOKgo2FY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importando findspark\n",
        "import findspark\n",
        "\n",
        "# Iniciando findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "4sEO98qJo9UZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importando o pacote necessário para iniciar uma seção Spark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# iniciando o spark context\n",
        "spark = SparkSession.builder.master('local[*]').getOrCreate()\n",
        "\n",
        "# Verificando se a sessão foi criada\n",
        "spark"
      ],
      "metadata": {
        "id": "EgNA9ErDo9XB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.catalog"
      ],
      "metadata": {
        "id": "uXZLvgMPo9ZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.catalog.listDatabases"
      ],
      "metadata": {
        "id": "8-8MYcewpK0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.catalog.listTables"
      ],
      "metadata": {
        "id": "Dc5f0WdmpK23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Acessando meu drive no Google drive**"
      ],
      "metadata": {
        "id": "N5fyZERapSMJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth , drive\n",
        "\n",
        "from googleapiclient.discovery import build\n",
        "drive_service = build('drive', 'v3')\n",
        "\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "4YGkN7MhpK40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Início do desafio final**"
      ],
      "metadata": {
        "id": "QfPHjHhrpflA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Lendo os dados**"
      ],
      "metadata": {
        "id": "lp9pXJmOpsF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# df_enem2020 = spark.read.csv('drive/MyDrive/EDC-M2/enem2020.tsv', header=True, sep='\\t')\n",
        "\n",
        "\n",
        "# Ler os dados do ENEM 2020\n",
        "enem = (\n",
        "    spark\n",
        "    .read\n",
        "    .format(\"csv\")\n",
        "    .option(\"header\", True)\n",
        "    .option(\"inferSchema\", True)\n",
        "    .option(\"delimiter\", \";\")\n",
        "    .load(\"drive/MyDrive/EDC-M2/enem2020.tsv\")\n",
        "    )\n",
        "    \n",
        "\n"
      ],
      "metadata": {
        "id": "7O-qKHMzo9b5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_enem2020.printSchema()"
      ],
      "metadata": {
        "id": "5EiNpDivp7mH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Total de registros**"
      ],
      "metadata": {
        "id": "q1lQw_Q3qER8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# df_enem2020.count()"
      ],
      "metadata": {
        "id": "ohz0G_DQp7ob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Gravando arquivo parquet no S3**"
      ],
      "metadata": {
        "id": "vPPpT7TVrbmC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Escrever os dados no datalake em formato parquet\n",
        "(\n",
        "    enem\n",
        "    .write\n",
        "    .mode(\"overwrite\")\n",
        "    .format(\"parquet\")\n",
        "    .partitionBy(\"year\")\n",
        "    .save(\"C:/Users/marcelo_sa/Documents/MBA Engenheiro de Dados Cloud/Desafio do Bootcamp/Datalake/\")\n",
        ")"
      ],
      "metadata": {
        "id": "BujHckGPp7qf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l-u4zs8Vrn_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TF6ibrObroBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Nj8yiO3oroD0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}